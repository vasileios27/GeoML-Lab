{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3eca50",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸ§° Tutorial 03 â€” Preprocessing for Machine Learning\n",
    "\n",
    "In this notebook, we convert ERA5 data into **MLâ€‘ready arrays**.  \n",
    "We'll: create **lag features**, reshape to `[samples, features]`, split **by time**, standardize, and save arrays.\n",
    "\n",
    "> **Tip:** Keep your dataset small while learning. Later, you can scale to larger files or multiple variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9115bc3c",
   "metadata": {},
   "source": [
    "## 0. Imports & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8cdae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: torch export (if installed)\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "except Exception:\n",
    "    TORCH_AVAILABLE = False\n",
    "\n",
    "DATA_PATH = Path(\"data/era5_sample.nc\")     # adjust if needed\n",
    "OUT_DIR = Path(\"data/processed\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TARGET_VAR = \"t2m\"   # variable to predict\n",
    "LAGS = [1, 2, 3]     # number of previous timesteps used as features\n",
    "TEST_FRAC = 0.15\n",
    "VAL_FRAC = 0.15      # train fraction = 1 - TEST_FRAC - VAL_FRAC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37977b91",
   "metadata": {},
   "source": [
    "## 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cffdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = xr.open_dataset(DATA_PATH)\n",
    "assert TARGET_VAR in ds.data_vars, f\"{TARGET_VAR} not found in dataset. Available: {list(ds.data_vars)}\"\n",
    "var = ds[TARGET_VAR]\n",
    "var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3935d8",
   "metadata": {},
   "source": [
    "## 2. Inspect time dimension and quick preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db77000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "times = var.time.values\n",
    "print(\"Timesteps:\", len(times))\n",
    "print(\"First:\", times[0] if len(times) else \"N/A\")\n",
    "print(\"Last :\", times[-1] if len(times) else \"N/A\")\n",
    "\n",
    "# Quick map of first timestep\n",
    "_ = var.isel(time=0).plot()\n",
    "plt.title(f\"{TARGET_VAR} (first timestep)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f2061",
   "metadata": {},
   "source": [
    "## 3. Build lag features for supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f049de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We'll create samples at times t where we have all lags (t-1, t-2, ...)\n",
    "max_lag = max(LAGS) if LAGS else 0\n",
    "if var.sizes.get(\"time\", 0) <= max_lag:\n",
    "    raise ValueError(\"Not enough timesteps to create lag features. Increase data length or reduce LAGS.\")\n",
    "\n",
    "# Align all lagged arrays by time\n",
    "lagged = []\n",
    "for k in LAGS:\n",
    "    lagged.append(var.shift(time=k).isel(time=slice(max_lag, None)))\n",
    "target = var.isel(time=slice(max_lag, None))\n",
    "\n",
    "# Stack space into a single dimension for ML\n",
    "def stack_space(da):\n",
    "    # Handle common ERA5 dims: time, latitude, longitude\n",
    "    spatial_dims = [d for d in da.dims if d not in (\"time\",)]\n",
    "    # Ensure consistent order\n",
    "    da_stacked = da.stack(space=spatial_dims)\n",
    "    return da_stacked\n",
    "\n",
    "lagged_stacked = [stack_space(da) for da in lagged]\n",
    "target_stacked = stack_space(target)\n",
    "\n",
    "# Drop any rows with NaNs across lags/target\n",
    "# Concatenate features along the feature axis (lags)\n",
    "X_list = [da.rename(f\"lag_{k}\") for da, k in zip(lagged_stacked, LAGS)]\n",
    "X = xr.concat(X_list, dim=\"feature\")  # dims: time, space, feature\n",
    "y = target_stacked  # dims: time, space\n",
    "\n",
    "# Move to numpy with samples axis first: [samples, features]\n",
    "# We'll collapse (time, space) -> samples\n",
    "X_np = X.transpose(\"time\", \"space\", \"feature\").values.reshape(-1, len(LAGS))\n",
    "y_np = y.transpose(\"time\", \"space\").values.reshape(-1)\n",
    "\n",
    "# Remove any rows where either X or y has NaNs\n",
    "mask = np.isfinite(X_np).all(axis=1) & np.isfinite(y_np)\n",
    "X_np = X_np[mask]\n",
    "y_np = y_np[mask]\n",
    "\n",
    "print(\"X shape (samples, features):\", X_np.shape)\n",
    "print(\"y shape (samples,):\", y_np.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba41d47",
   "metadata": {},
   "source": [
    "## 4. Time-based train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac11560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For a true time-based split, we need indices per time step, not shuffled samples.\n",
    "# We'll recompute using per-time slicing then flatten space.\n",
    "\n",
    "def to_samples_by_time(var, lags):\n",
    "    max_lag = max(lags) if lags else 0\n",
    "    # Build per-time feature arrays\n",
    "    feats = []\n",
    "    for k in lags:\n",
    "        feats.append(var.shift(time=k))\n",
    "    # Valid time range\n",
    "    valid = slice(max_lag, None)\n",
    "    feats = [f.isel(time=valid) for f in feats]\n",
    "    tgt   = var.isel(time=valid)\n",
    "\n",
    "    def stack_space(da):\n",
    "        spatial_dims = [d for d in da.dims if d != \"time\"]\n",
    "        return da.stack(space=spatial_dims)\n",
    "\n",
    "    feats = [stack_space(f) for f in feats]\n",
    "    tgt   = stack_space(tgt)\n",
    "\n",
    "    # Convert to numpy: time, space -> flatten space but keep time\n",
    "    F = np.stack([f.values for f in feats], axis=-1)  # shape: time, space, num_lags\n",
    "    T = tgt.values                                   # shape: time, space\n",
    "\n",
    "    return F, T  # keep time dimension\n",
    "\n",
    "F, T = to_samples_by_time(var, LAGS)\n",
    "time_len, space_len, num_feats = F.shape\n",
    "print(\"Per-time shapes:\", F.shape, T.shape)\n",
    "\n",
    "# Split indices by time\n",
    "n_test = int(np.floor(TEST_FRAC * time_len))\n",
    "n_val  = int(np.floor(VAL_FRAC  * time_len))\n",
    "n_train = time_len - n_val - n_test\n",
    "\n",
    "idx_train = slice(0, n_train)\n",
    "idx_val   = slice(n_train, n_train + n_val)\n",
    "idx_test  = slice(n_train + n_val, time_len)\n",
    "\n",
    "def flatten_time_slice(F, T, slc):\n",
    "    Fx = F[slc].reshape(-1, F.shape[-1])\n",
    "    Ty = T[slc].reshape(-1)\n",
    "    mask = np.isfinite(Fx).all(axis=1) & np.isfinite(Ty)\n",
    "    return Fx[mask], Ty[mask]\n",
    "\n",
    "X_train, y_train = flatten_time_slice(F, T, idx_train)\n",
    "X_val,   y_val   = flatten_time_slice(F, T, idx_val)\n",
    "X_test,  y_test  = flatten_time_slice(F, T, idx_test)\n",
    "\n",
    "print(\"Train:\", X_train.shape, y_train.shape)\n",
    "print(\"Val  :\", X_val.shape, y_val.shape)\n",
    "print(\"Test :\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95df9908",
   "metadata": {},
   "source": [
    "## 5. Standardize features (fit on train, apply to val/test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a05ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mu = X_train.mean(axis=0, keepdims=True)\n",
    "sigma = X_train.std(axis=0, keepdims=True)\n",
    "sigma[sigma == 0] = 1.0  # avoid div by zero\n",
    "\n",
    "def standardize(X, mu, sigma):\n",
    "    return (X - mu) / sigma\n",
    "\n",
    "X_train_std = standardize(X_train, mu, sigma)\n",
    "X_val_std   = standardize(X_val,   mu, sigma)\n",
    "X_test_std  = standardize(X_test,  mu, sigma)\n",
    "\n",
    "print(\"Means (per feature):\", mu.ravel())\n",
    "print(\"Stds  (per feature):\", sigma.ravel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde8f71c",
   "metadata": {},
   "source": [
    "## 6. Save arrays to `data/processed/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8836e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save(OUT_DIR / \"X_train.npy\", X_train_std)\n",
    "np.save(OUT_DIR / \"y_train.npy\", y_train)\n",
    "np.save(OUT_DIR / \"X_val.npy\",   X_val_std)\n",
    "np.save(OUT_DIR / \"y_val.npy\",   y_val)\n",
    "np.save(OUT_DIR / \"X_test.npy\",  X_test_std)\n",
    "np.save(OUT_DIR / \"y_test.npy\",  y_test)\n",
    "\n",
    "np.savez(OUT_DIR / \"stats.npz\", mu=mu, sigma=sigma, lags=np.array(LAGS))\n",
    "\n",
    "print(\"Saved:\")\n",
    "for p in sorted(OUT_DIR.glob(\"*\")):\n",
    "    print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784a717c",
   "metadata": {},
   "source": [
    "## 7. (Optional) Export as PyTorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c50d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TORCH_AVAILABLE:\n",
    "    torch.save(torch.tensor(X_train_std, dtype=torch.float32), OUT_DIR / \"X_train.pt\")\n",
    "    torch.save(torch.tensor(y_train,     dtype=torch.float32), OUT_DIR / \"y_train.pt\")\n",
    "    torch.save(torch.tensor(X_val_std,   dtype=torch.float32), OUT_DIR / \"X_val.pt\")\n",
    "    torch.save(torch.tensor(y_val,       dtype=torch.float32), OUT_DIR / \"y_val.pt\")\n",
    "    torch.save(torch.tensor(X_test_std,  dtype=torch.float32), OUT_DIR / \"X_test.pt\")\n",
    "    torch.save(torch.tensor(y_test,      dtype=torch.float32), OUT_DIR / \"y_test.pt\")\n",
    "    print(\"Saved PyTorch tensors.\")\n",
    "else:\n",
    "    print(\"PyTorch not installed â€” skipping tensor export.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9557a4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## âœ… You now have MLâ€‘ready data!\n",
    "- Feature matrix with **lagged inputs**\n",
    "- Target vector aligned in time\n",
    "- Proper **time-based** train/val/test split\n",
    "- Standardized features and saved arrays\n",
    "\n",
    "**Next:** Tutorial 04 â€” Build a basic ML model (PyTorch) using these arrays.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
